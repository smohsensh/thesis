%------chapter 2: literature review
\chapter{روش‌های پیشین}

استفاده از اشتراک و تمایز برخی ویژگی‌ها میان دسته‌های مختلف در بینایی ماشین مورد بررسی قرار گرفته است
\cite{BakkerH03, TsochantaridisJHA05, ulman2005}
اما این روش‌ها به شناسایی دسته‌های کاملا جدید کمتر توجه نشان داده‌اند.

مسئله‌ی یادگیری تک‌ضرب
\LTRfootnote{One-shot Learning}
یک مسئله نزدیک به یادگیری صفر است که پیش معرفی مسئله یادگیری از صفر مورد بررسی بوده است
\cite{miller12}.
در حقیقت می‌توان یادگیری تک‌ضرب را حالت خاصی از یادگیری از صفر در نظر گرفت که در آن توصیف دسته‌های دیده نشده به صورت یک نمونه از آن دسته ارائه شده است
\cite{bengio08}.

 پدیده شروع سرد
\LTRfootnote{cold start}
 در سامانه‌های توصیه‌گر
\LTRfootnote{Recommender Systems}
 را نیز می‌توان از حالت‌های خاص یادگیری بدون برد در نظر گرفت که در آن برای یک کاربر یا مورد جدید پیشنهاد صورت می‌گیرد.


بیان مسئله  یادگیری بدون برد به طور رسمی برای اولین بار در
\cite{bengio08}
صورت گرفت. در آن‌جا دو رویکرد کلی برای حل مسئله یادگیری از صفر بیان می‌شود. یک روش که رویکرد فضای ورودی
\LTRfootnote{input space view}
نامیده می‌شود، سعی در مدل کردن نگاشتی با دو ورودی دارد. یکی نمونه‌ها و دیگری توصیف دسته‌ها. این نگاشت برای نمونه‌ها و توصیف‌های مربوط به یک دسته امتیاز بالا و برای نمونه‌ها و توصیفاتی که متعلق به دسته‌ی یکسانی نیستند مقادیر کوچکی تولید می‌کند. با تخمین زدن چنین نگاشتی روی داده‌های آموزش، دسته‌بندی نمونه‌های آزمون در دسته‌هایی که تا کنون نمونه‌ای نداشته‌اند ممکن خواهد شد. به این صورت که هر نمونه با توصیف دسته‌های مختلف به این تابع داده شده و متعلق به دسته‌ای که امتیاز بیشتری بگیرد، پیش‌بینی خواهد شد.
در روش دیگر که رویکرد فضای مدل
\LTRfootnote{model space view}
نام دارد، مدل مربوط به هر دسته (برای مثال پارامترهای دسته‌بند مربوط به آن)، به عنوان تابعی از توصیف آن دسته در نظر گرفته می‌شود.

ما در این فصل از دسته‌بندی دیگری برای مرور روش‌های پیشین استفاده می‌کنیم. برای این کار ابتدا معرفی یک چارچوب کلی برای انجام یادگیری بدون برد لازم است. دو رویکرد فوق نیز در این چارچوب قابل بیان هستند، این موضوع در بخش \ref{} که مثال‌هایی از این رویکردها مرور می‌شود، روشن‌تر خواهد شد.
 % چارچوبی که در ادامه می‌آید بر این اساس استوار است که تصاویر و توصیفات آن‌ها به یک فضای مشترک نگاشته می‌شوند. اگر بخواهیم دو دسته‌ی بالا در را در با این بیان توصیف کنیم، در رویکرد فضای ورودی، فضای مشترک فضایی است که نگاشت شباهت سنجی، ضرب داخلی آن فضاست و در رویکرد فضای مدل، فضای مشترک فضای دسته‌بندها خواهد بود.

 می‌توان گفت که هر روش برای یادگیری بدون برد از سه قسمت تشکیل شده است که ممکن است به صورت مستقل یا همزمان انجام شوند؛ این سه قسمت عبارتند از:
\begin{enumerate}
  \item یادگرفتن نگاشتی از فضای تصاویر به فضای مشترک
  که آن را با $\psi$ نشان می‌دهیم.
  \item نگاشت توصیف‌ها به فضای مشترک
  که آن را با $\phi$ نشان می‌دهیم.
  \item اختصاص برچسب به تصاویر
\end{enumerate}
در ادامه به بررسی روش‌های ارائه شده برای مسئله یادگیری بدون برد با استفاده از چارچوب ارائه شده خواهیم پرداخت.
\section{پیش‌بینی ویژگی \LTRfootnote{Attribute Prediction}}
این دسته از روش‌ها عموما به حالتی از مسئله یادگیری از صفر تعلق دارند که توصیف دسته‌ها از نوع بردار ویژگی باشد. در این حالت فضای مشترک همان فضای ویژگی‌ها در نظر گرفته می‌شود. به عبارت دیگر نگاشت $\psi$ نگاشت همانی فرض شده و یادگرفته نخواهد شد. روش‌های اولیه ارائه شده برای یادگیری بدون برد از نوع یادگیری از صفر بودند و پس از آن‌ هم قسمت قابل توجهی از روش‌ها در این دسته جای می‌گیرند که در ادامه آن‌ها را به تفصیل مرور می‌کنیم.

\subsection{پیش‌بینی ویژگی مستقیم و غیر مستقیم}
در
\cite{lampert09}
با فرض این که ویژگی‌ها به صورت مستقل از یکدیگر قابل پیش‌بینی هستند دو رویکرد برای این کار ارائه می‌کند. پیش‌بینی ویژگی مستقیم و پیش‌بینی ویژگی غیر مستقیم. مدل گرافی مورد استفاده در این دو رویکرد در تصویر \ref{fig:dap} آمده است. در پیش‌بینی ویژگی مستقیم برچسب‌ها به شرط دانستن ویژگی‌های درون تصویر، از تصویر مستقل هستند. در این روش برای هر یک ویژگی‌ها یک دسته‌بند یاد گرفته می‌شود. با توجه به این که ویژگی‌ها برای تصاویر آزمون معین هستند این کار با استفاده از یک دسته‌بند احتمالی برای هر ویژگی قابل انجام است. در نهایت احتمال تعلق هر یک از برچسب‌های
$ u \in \mathcal{U} $
با استفاده از رابطه زیر بدست خواهد آمد.
\begin{equation}
  P(z_u | x ) = \sum_{c\in {0,1}^a} P(u | c) p(c|x)
\end{equation}
از با توجه به فرض استقلال ویژگی داریم
$P(c|x) = \prod_{n=1}^a P(c_m |x)$.
برای محاسبه جمله $P(z_u | a)$ از قانون بیز استفاده می‌کنیم:
\[
P(u | c) = \frac{P(u)P(c|u)}{P(a^{u})}  = \frac {P(u) \mathds{1}(c= c^{u})} {P(c^{u})}
\]
با جایگذاری در رابطه \mathref{eq:dap0} خواهیم داشت:
\begin{equation}
  P(u | x ) = \frac{P(u)}{P(c^{u})} \prod_{n=1}^a P(a^{u}_n|x)
\end{equation}
در نهایت برچسبی که احتمال فوق را بیشینه کند، پیش‌بینی مربوط به تصویر $x$ خواهد بود.

در روش پیش‌بینی ویژگی غیر مستقیم، IAP
\LTRfootnote{Indirect Attribute Prediction}
 تخمین  $P(c_i|x) $ تغییر داده می‌شود؛ به این صورت که ابتدا یک دسته‌بند چند دسته‌ای یعنی $P(y_k |x)$ روی داده‌ها یاد گرفته می‌شود و سپس رابطه ویژگی‌ها و برچسب‌ها به صورت قطعی مدل می‌شود:
\begin{equation}
P(c_i | x) = \sum_{k=1}^s P(y_k | x) \mathbb{I}(c_i = c^{y_k}_i)
\end{equation}
در نهایت در هر دو روش برچسب نهایی با تخمین MAP 
\LTRfootnote{Maximum a Posteriori}
از رابطه زیر تعیین می‌شود:
\begin{equation}
\hat{y} = \argmax_{u \in \mathcal{U}} P(u|x) =  \argmax_{u \in \mathcal{U}} \prod_{i=1}^a \frac{P(c_i^u | x)}{P(c_i^u)}
\end{equation}
روش ارائه شده در 
\cite{suzuki14}
مشابه همین روش است با این تفاوت که احتمال مشاهده هر کدام ویژگی‌ها را هم در محاسبه دخیل می‌کند تا با وزن‌های متفاوت با توجه به اهمیتشان در دسته‌بندی نقش داشته باشند. ضعف بزرگ این روش‌ها فرض مستقل بودن ویژگی‌ها از یکدیگر است؛ چرا که این فرض در مسائل واقعی معمولا بر قرار نیست. برای مثال زمانی که ویژگی آبزی بودن برای یک موجود در نظر گرفته می‌شود احتمال ویژگی پرواز کردن برای آن بسیار کاهش می‌یابد. مدل‌های گرافی برای در نظر گرفتن وابستگی‌های میان ویژگی‌ها به کار گرفته شده‌اند. نویسندگان \cite{topicmodel} برای در نظر گرفتن ارتباط بین ویژگی‌ها و ارتباط ویژگی‌ها با برچسب نهایی روش‌های مدل‌سازی موضوع \LTRfootnote{  Topic Modeling} را از حوزه یادگیری در متن اقتباس می‌کنند. همچنین  نویسندگان \cite{unified13} برای این کار یک چارچوب بر اساس مدل‌های گرافی احتمال معرفی می‌کنند. در این چارچوب یک شبکه بیزی\LTRfootnote{  Baysian Network}  برای مدل کردن این روابط در نظر گرفته می‌شود و ساختار آن که نشان‌دهنده وابستگی یا استقلال ویژگی‌ها با هم یا با برچسب است، با کمک روش‌های یادگیری ساختار 
\LTRfootnote{Structure Learning}
شناخته می‌شود. 

\section{نگاشت‌های دو خطی}
حالت دیگری از چارچوب کلی معرفی شده در ابتدای فصل این است که نگاشت به فضای مشترک یک نگاشت دوخطی باشد. یعنی به این صورت که $W$ نگاشتی خطی است که $x^TW$ تصویر $x$ را به فضای توصیف‌ها نگاشته و $Wc$ توصیف $c$ را به فضای تصاویر می‌نگارد. در این حالت، این که فضای مشترک در حقیقت کدام یک از فضاهای تصاویر یا توصیفات هستند، جواب روشنی ندارد. نقطه‌ی قوت این روش‌ها در امکان پیچیده‌تر کردن تابع هزینه است. چرا که در حالتی که نگاشت خطی است مسائل بهینه‌سازی پیچیده‌تری نسبت به حالت غیر خطی قابل حل خواهند بود. یک انتخاب متداول برای تابع هزینه، توابع رتبه‌بند 
 \LTRfootnote{ranking function}
هستند. با توجه به این که عموما بعد از یادگیری این نگاشت، دسته‌ای که نزدیک‌ترین توصیف را (با معیاری مثل فاصله یا ضرب داخلی) دارد، به عنوان پیش‌بینی تولید می‌شود،
 چنین تابع هزینه‌ای یک انتخاب طبیعی است. چرا که مسئله‌ی نزدیکترین همسایه در اصل یک مسئله رتبه‌بندی
 است و استفاده از یک تابع هزینه‌ی رتبه‌بند برای یادیگری نگاشت بهتر از مجموع مربعات است که تنها فاصله نقاط از برچسب خودشان را در نظر می‌گیرد \cite{devise}. 
 
در
\cite{akata2013}
 تابع هزینه رتبه‌بند WSABIE 
\cite{Weston2010}
که برای حاشیه‌نویسی تصاویر پیشنهاد شده، به مسئله یادگیری بدون برد انطباق می‌دهد. 
تابع هزینه WASBIE به این صورت تعریف شده است:

\begin{align}
L(x_s, Y_s ; W, \theta) = \frac{1}{N_s} \sum_{n=1}^{N_s} \lambda_{r_\Delta (x_n, y_n)} \sum_{y \in \mathcal{Y}} \max (0, \mathit{l}(x_n, y_n, y) ) \\
\mathit{l}(x_n,y_n,y) = \mathds{1}(y \neq y_n) + \phi(x_n)^TW \theta(y) - \phi(x_n)^TW\theta(y_n)
\end{align}

که در آن 
$ r_\Delta (x_n, y_n) =  \sum_{y \in \mathcal{Y}} \mathbb{I}(\mathit{l}(x_n, y_n, y)  > 0) $ 
 و $\lambda_k$ یک تابع نزولی از $k$ است. این تابع، پیش‌بینی اشتباه ویژگی‌ها را  این گونه جریمه می‌کند که به ازای برچسب نادرستی که رتبه بالاتری از برچسب صحیح در دسته‌بندی دریافت کرده، جریمه‌ای متناسب با امتیاز برچسب ناصحیح در نظر گرفته می‌شود.ضریب نزولی $\lambda_k$ میزان جریمه را برای برچسب‌های غلط در رتبه‌های بالا، بیشتر در نظر می‌گیرد. در انطباق برای یادگیری بدون برد، بهینه‌سازی تنها روی نگاشت $W$ انجام شده و  تابع $\theta$ دانسته فرض می‌شود: 
$\thata(y) = c_y$.


در \cite{SJE} ...
%TODO

یک نحوه‌ی استفاده دیگر از نگاشت‌های دو خطی، دسته‌بندی مستقیم با این نگاشت است. 
\begin{equation} 
\minimize_{W \in \mathbb{R}^{d \times a}} \normf{X_s^T WC_s - Y} + \Omega(W) \label{eq:emb}
\end{equation}
که در آن $\Omega$ یک جمله منظم‌سازی است.
در این حالت اگر تبدیل را از فضای تصاویر به فضای ویژگی‌ها نگاه کنیم، نگاشت $W$ باید تصاویر را به زیرفضایی عمود به تمامی بردار ویژگی‌های مربوط به برچسب‌های نادرست بنگارد. 
عملکرد خوب این روش، با وجود استفاده از تابع هزینه ساده مجموع مربعات خطا که در یادگیری ماشین تابع هزینه‌ی مناسبی برای دسته‌بندی به شمار نمی‌آید، به جمله منظم سازی آن نسبت داده می‌شود. جمله منظم‌سازی $\Omega$ به این صورت تعریف می‌شود:
\begin{equation} \label{eq:emb_reg}
\Omega(W) = \lambda \normf{WC_s} + \gamma \normf{X_s^T W}  + \lambda \gamma \normf{W}
\end{equation}
این جمله منظم‌سازی با دیدگاه نگاشت دوخطی طبیعی است. چرا که $WC_S$ را می‌توان یک دسته‌بند خطی روی فضای تصاویر در نظر گرفت و از طرفی $X_s^T W$ یک دسته‌بند روی بردارهای ویژگی است در نتیجه طبیعی است که پارامترهای این دو دسته بند با نرم فروبنیوس آن‌ها کنترل شود. 
استفاده از توابع نرم دوم برای خطا و منظم‌سازی در این روش باعث شده است که مسئله بهینه‌سازی جواب به صورت فرم بسته داشته باشد و زمان اجرا نسبت به سایر روش‌ها بسیار کمتر باشد. 
این روش در 
\cite{lessismore}
برای توصیفات متنی توسعه داده شده است. با توجه به ابعاد بالای داده‌های متنی و همچنین نویز زیادی که در آن‌ها در مقایسه با بردارهای ویژگی وجود دارد، ماتریس تبدیل $W$ به دو ماتریس تجزیه می‌شود:
\begin{equation}
$W = V_x^T V_c$
\end{equation}
با این تجزیه از افزایش شدید تعداد پارامترها در اثر افزایش بعد بردار توصیف‌ها جلوگیری می‌شود. (دقت کنید که بعد $C$ برابر $d\times a$ است) علاوه بر این $V_c$ می‌تواند برای حذف قسمت‌های نویزی $C$ به  کار گرفته شود و $V_x$ مانند $W$ در حالت اصلی عمل کند یعنی پارامترهای یک دسته‌بند را از روی توصیف‌ها تولید کند. در نهایت تابع هزینه برای این روش به صورت زیر تعریف می‌شود:
\begin{equation}
\min_{V_x, V_c} \normf{X_s^T + V_x^T V_c C}^2 + \lambda_1 \normf{ V_x^T V_c C} + 
\lambda_2 \norm{V_c^T}_{2,1}
\end{equation}
که
$\norm{M^T}_{2,1} = \sum_i \norm{M_{(i)}}_2 $
و این نوع منظم‌سازی، ستون‌های ماتریس $V_c$ را به سمت تنک بودن سوق خواهد داد. در واقع اگر $\lambda_2$ بزرگ انتخاب شود، $V_c$ نقش یک ماتریس انتخاب ویژگی 
\LTRfootnote{feature selection}
را خواهد داشت. جمله‌های منظم سازی دیگر در 
\ref{eq:emb_reg}
به دلیل تاثیر اندکشان در آزمایشات عملی حذف شده‌اند. 

در 
\cite{devise}
نیز که برای اولین بار توصیف تنها نام برچسب دسته‌ها در نظر گرفته شده، از نگاشت دو خطی استفاده شده است. در این روش نام برچسب‌ها با استفاده از مدل نهان‌سازی کلمات word2vec
\cite{word2vec}
کلمات به بردارهایی نگاشته می‌شوند. ابعاد فضای نهان‌سازی کلمات یک فراپارامتر است که در این مقاله با اعتبار سنجی تعیین شده است. استخراج ویژگی از  تصاویر  با استفاده از شبکه عصبی کانولوشنال 
\cite{alexnet}
که روی دسته‌های دیده شده آموزش داده شده، انجام می‌شود. در نهایت یک تابع بیشترین حاشیه
\LTRfootnote{Max margin}
برای یادگیری نگاشت دو خطی پیشنهاد می‌شود.
\begin{equation}
 L((x_n, y_n);W) = \sum_{y\neq y_n} \max(0, \xi  - x_nWc_{y_n} + x_nWc_y) 
\end{equation}
که در آن $\xi$ حاشیه دسته‌بندی است. دسته‌بندی نمونه‌های جدید با نگاشتن $x$ به فضای برچسب‌ها و استفاده از دسته‌بند نزدیکترین همسایه صورت می‌گیرد. 
 در این پژوهش گزارش شده که نتایج چنین تابع هزینه‌ای بهتر از تابع هزینه مجموع مربعات بوده است، چرا که مسئله‌ی نزدیکترین همسایه در اصل یک مسئله رتبه‌بندی
 \LTRfootnote{ranking}
 است و استفاده از یک تابع هزینه‌ی رتبه‌بند برای یادیگری نگاشت بهتر از مجموع مربعات است که تنها فاصله نقاط از برچسب خودشان را در نظر می‌گیرد. 
 
 
 
 
