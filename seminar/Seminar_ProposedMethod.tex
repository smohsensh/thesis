\section{روش ارائه شده}\label{proposed}
در
\cite{hint14} 
 نشان داده‌شد که خروجی لایه‌ی سافت‌مکس 
\LTRfootnote{softmax}
شبکه‌های عصبی اطلاعاتی بیش از تنها یک دسته‌بند یکی‌یک در خود دارد. این اطلاعات می‌تواند برای بیان یک نمونه از دسته‌های دیده‌نشده بر اساس نسبت‌هایی از دسته‌های دیده شده استفاده شود. اگر خروجی چنین لایه‌ای را به ورودی $z$ (که خروجی لایه‌ی قبل است) برابر $q$ باشد، خواهیم داشت:
 
\begin{equation} \label{softmax}
	q_i = \frac{exp(z_i/T)}{\sum_j exp(z_j/T)}
\end{equation}
در زمان آموزش شبکه مقدار $T=1$ در نظر گرفته می‌شود، در این حالت تابع سعی می‌کند یکی از ابعاد ورودی که مربوط به دسته‌ی پیش‌بینی شده است را یک و سایر ابعاد را صفر کند بعبارتی مقدار $q_i$ بیشینه اختلاف زیادی با سایر $q_j$ها خواهد داشت. با افزایش مقدار $T$ که از آن به نام دما نیز یاد می‌شود، اختلاف پدید آمده نرم‌تر می‌شود و مقدار بعد بیشینه به سایر ابعاد نزدیک می‌شود. در روش پیشنهادی ما از چنین لایه‌ای برای نگاشت تصاویر آزمون به هستوگرام‌هایی از دسته‌های دیده‌شده استفاده می‌کنیم. به این صورت که ابتدا یک شبکه‌ عصبی چند لایه با اتصالات چگال و فعال‌سازی
\LTRfootnote{Activation}
 سافت‌مکس در لایه آخر به عنوان یک دسته‌بند روی دسته‌های دیده‌شده آموزش داده می‌شود. سپس از همین شبکه بدون تغییری در وزن‌های یادگرفته شده و تنها با جایگیزین کردن فعال‌سازی لایه‌ی آخر با تابع \eqref{softmax} با مقدار  $T>1$ برای نگاشت تصاویر آزمون به فضای دسته‌های دیده‌شده استفاده می‌شود.
برای فعال‌سازی لایه‌های پایین‌تر از واحد تصحیح خطی
\LTRfootnote{Rectified Linear Unit (ReLU)}
 استفاده شده است. هم‌چنین بین تمامی لایه‌های میانی منظم‌سازی حذف تصادفی
\LTRfootnote{Dropout}
وجود دارد که احتمال حذف آن با اعتبارسنجی متقابل تعیین می‌شود.

  تعداد لایه‌های شبکه و اندازه هر لایه برای هر مجموعه داده با اعتبار سنجی متقابل
 \LTRfootnote{Cross Validation} تعیین می‌شود.
  مقدار $T$ نیز  با اعتبار سنجی متقابل به صورت حذف بعضی از دسته‌های دیده شده از جریان آموزش و محک زدن مدل با آن‌ها به عنوان دسته‌های دیده‌نشده قابل تعیین است اما در آزمایش‌های انجام شده مقدار آن را ثابت برابر ۲۰ در نظر گرفته‌ایم. 
 
 برای نگاشت بردارهای ویژگی به هیستوگرام‌هایی از دسته‌های دیده‌شده، از مجموع  عکس فاصله‌های اقلیدسی و بلوکی
 \LTRfootnote{Manhattan Distance}
  بردارهای ویژگی استفاده شده است:
 \begin{equation} 
\psi_i(c^u) = \sum_{u \neq y} \frac{1}{\norm{c_i^u - c_i^y}_2 + \norm{c_i^u - c_i^y}_1}
\end{equation}
در نهایت دسته‌بندی به صورت دسته‌بندی نزدیک‌ترین همسایه (با فاصله اقلیدسی) انجام می‌شود.

