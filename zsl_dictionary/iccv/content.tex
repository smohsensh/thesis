
\begin{abstract}
One line of research to perform object recognition when labeled data is not available for all categories, is zero-shot learning,
in which a form of auxiliary information describing categories to help the recognition task.
In this paper we propose a semi-supervised framework, which is essentially a clustering problem over unseen instances. We further
use a linear mapping inspired by dictionary learning to represent the auxiliary information as visual features, making label prediction
possible using nearest neighbor. The clustering term is effective in exploiting the information provided by rich deep visual features and
mitigating the domain shift problem known to impair zero-shot recognition performance. We demonstrate this through extensive
experiments on four public benchmark which results improving state of the art prediction accuracy on three of them.
\end{abstract}


\section{Introduction}
Zero-shot learning is an extension to the conventional supervised learning scenario.
It introduces task of recognizing instances from categories with no training sample.
 The problem addressed by zero-shot learning rises naturally in practice wherever it is not feasible to acquire abundant labeled instances
 for all categories. Examples include large-scale and fine-grained classification.
Describing the task more precisely, in training phase labeled samples from some categories are provided,
these categories are called seen categories. There are also some other categories without labeled instances; these are called unseen categories.
There also exists some sort of description for every category that is sometimes called \textit{class signatures}.
Signatures may be a set of human-annotated discriminative attributes or derived from textual description available online.
In the test phase, unlabeled instances should be classified into seen or unseen categories. In this work, however, we focus on the most popular version
in which test instances belong only to unseen categories.
 %In this setting, zero-shot learning mitigates the problem of acquiring labeled instances, which can be hard for a novel, rare or fine-grained categories.

Most existing methods for zero-shot learning focus on using labeled images to learn a compatibility function indicating how similar an image is
to each label embedding. Each instance will then be labeled with the category having the most compatible signature.

 On the other hand, recent advances in deep convoloutional neural networks provide rich visual features with high discrimination capability.
 Thus, there is usually a significant structure in visual features domain
that can be used to help classification be done more precisely, like in semi-supervised settings. However little attention
has been paid to exploiting the information available in the distribution of images themselves in the context of zero-shot learning. We will show through experiment that
the space of images is indeed a rich space in which instances categories form natural clusters.
 %In the previous works, most concentration has been on finding embeddings for visual features and little effort has been made to exploit unsupervised information in images domain.
 In this work
 %we first justify the claim made above by applying a simple clustering algorithm and reporting
%Rand Index measure between cluster assignments and ground truth labels. Motivated by the result,
 we first propose a
  semi-supervised clustering algorithm to leverage labels available for seen classes to obtain more accurate cluster assignments.
 Then we use a simple dictionary learning method to map class descriptions to visual features, creating a set of landmarks in the visual domain.
 These landmarks are used to label cluster centers driven by clustering algorithm.
  We argue that this learning scheme can mitigate domain shift problem \cite{eccv14} between seen and unseen categories and thus achieve higher classification accuracy.
In Sec. \ref{experiments} we present experimental results on four popular zero-shot classification benchmarks to see
the method above outperforms the state of the art on three of them.
 We extend our work further and propose a framework to learn the dictionary and cluster (class label) assignments jointly
 improving results obtained from our first method.

The rest of paper is organized as follow. In Sec.\ref{related} we review existing methods for zero-shot recognition.
In Sec.\ref{proposed} we present our first method consisting of a semi-supervised clustering algorithm and a way to label cluster, In Sec.\ref{joint} we
we define our framework for performing clustering and prediction jointly, In Sec.\ref{experiments} we report our experiments and in Sec.\ref{conclusion}
we conclude.
\section{Related Work} \label{related}
Most existing methods for zero-shot object recognition can be described as finding a compatibility function scoring how
similar an image and a description are.
or with more fine-granularity as follows:
\begin{itemize}
  \item Find (or use existing) embeddings for class labels in a semantic space.
  \item Map images into that semantic space.
  \item Classify images in the semantic space (usually using a nearest neighbor classifier or label propagation).
\end{itemize}
these steps may be done independently or jointly.
A notable body of work in zero-shot recognition belongs to attribute prediction  \cite{lampert09, topicmodel, ajoint11, unified13, suzuki14}.
In these methods, the semantic label embeddings are considered to be externally provided attributes. So label embeddings are
already available and the task is to map images to the semantic space i.e. predicting attributes.
Early methods like \cite{lampert09} assume independence between attributes and train binary attribute classifiers.
Probabilistic graphical models have been utilized to model and/or learn correlations among different attributes \cite{topicmodel, unified13}, to improve the prediction.
In \cite{jayaraman14}  a random forest approach has been employed that accounts for unreliability in attribute predictions
 for final class assignment.

%transfer learning in transductive settings

More recent works, exploit bilinear mapping to embed images into semantic space and assign a compatibility score to image and label embedding
 pairs based on their inner product. Different objective functions have been proposed for learning such bilinear mapping.
 \cite{emb15} proposed a simple sum of squared error but with better regularization.
 \cite{Akata2015} learn a bilinear compatibility function between images and thei label embedding, using a max-margin objective function, they
 extend they works further to nonlinear compatibilty functions that can be expressed as a mixture of bilinear mappings \cite{Xian2016}.
\cite{li15max} define a max-margin objective function similar to structured SVM objective.
 to find the mapping. \cite{li15max, semi15} also use a max margin objective function
while the latter does not fix the label embeddings to externally provided ones in order to obtain more discriminative label embeddings. These two methods
differ from other existing methods in this way that embeddings and labels for test instances are learned simultaneously. This provides the possibility of
leveraging unsupervised information in test images for example as done in \cite{semi15} by using a Laplacian regularization term that penalizes
 similar objects assigned to different classes.
Our work differs with these two methods in many ways. They
learn a max margin classifier on the image space calssifying both seen and usneen instances while We use ridge regression to map signatures to image space, Resulting
in a much simpler optimization problem. We also explicitly account for domain shift problem in our objective function and thus achieving better results.


 % and ability to use more discriminative high-dimensional deep features.

 In absence of human annotated attributes, text from online encyclopedias or just the name of the class is used. Some existing method
 first do a feature extraction on these auxiliary information turning them to vectors that can be treated analogous to attribute vectors.
 \cite{devise} use a bilinear mapping as compatibility function among deep visual features and Word2vec \cite{word2vec} representation of
 class names. \cite{ba2015} uses a nonlinear mapping defined by a neural network to model compatibility.
  \cite{mohamed13} combines regression with knowledge transfer and proposes an objective to predict classifier parameters
 for unseen categories. \cite{Qiao2016} modifies the work of \cite{emb15} to fit with the textual descriptions.
 \cite{Fu2016} uses a large set of vocabulary and learns the mapping from images to word embeddings maximinzing
 the margin with respect to all words in the category; this framework can be used in zero-shot and also supervised and open set learning problems.
 \cite{Akata2016} proposes to use multiple auxilary information and also  part annotation in image domain,
 to compensate for weaker supervision in textual data.

 Designing label embedding is another line of research that can also be used for zero-shot recognition. For a label embedding to be helpful in classification task,
 it should exhibit two properties \cite{Yu2013}: 1) category-separability 2) Learnability. In \cite{Yu2013} an objective function is defined to derive
 such label embeddings based on information about similarities among categories.

A relatively popular embedding for labels is the to describe unseen categories as how similar they are to seen ones.
\cite{costa} construct  classifier for an unseen category by linear combination of seen class classifiers
using this kind of representation as mixing weights. \cite{convex} use output from softmax layer of a CNN to get the similarity scores.
Using these as weights they represent images in the semantic space as convex combination of seen label embeddings.
\cite{sse} also uses histogram of seen class proportions as label embedding and then define a max margin framework to embed images
 in this space. The authors extend their work further \cite{agnostic} and formulate a supervised dictionary learning
 for learning embeddings jointly.


There are major differences between our work and  ~\cite{Kodirov2015} which also uses a dictionary learning scheme in which coding coefficients is considered to be
 label embeddings in semantic space. In that work a sparse coding objective with two regularization terms designed specifically for
 zero-shot learning are used to map images into semantic space. Most importantly, label predictions
 for unseen instances appear directly in our objective function while in \cite{Kodirov2015} the prediction is accomplished using
 nearest neighbor or label propagation on embeddings of images in attribute space. Moreover we only learn the mapping
 on seen instances for which the attributes i. e. the dictionary coefficient are already available and
 do not learn embedding for test instances in attribute space but map test signatures to image space.

\section{Proposed Approach [Or a name maybe :-?]} \label{proposed}
In this section we first ...


\subsection{Notation}
$X, \mathbf{x}, x$ denote matrices, column vectors and scalars respectively. $\norm{X}$ denotes the Frobenius norm of a matrix and
$X_{(i)}$ denotes its $i$th column.
Suppose there are $n_s$ seen categories and $n_u$ unseen categories. For each category y,
auxiliary information $a_y \in \mathcal{R}^r$ is available. We assume that labels $\{1, \ldots, n_s \}$
correspond to seen categories.

We use $X_s \in \mathbb{R}^{d \times N_s}$ and $X_u \in \mathbb{R}^{d \times N_u}$
to denote matrices whose columns are seen and unseen images respectively where $d$ is the dimension of image features.
$S_s = [a_1, \ldots, a_{n_s}]$ denotes the matrix of seen class signatures. $S_u$ is defined similarly.
$Z_s = [ mathbf{z_1, \ldots, z_{N_s}} ]$
denotes train data labels in one hot encoding format.

\subsection{Clustering Method}
Our first method can be roughly summarized in three steps:
\begin{enumerate}
  \item Using data from seen classes we learn a linear mapping from  attribute vectors to images like in \cite{Kodirov2015}
  \item We cluster data using our proposed clustering algorithm
  \item We then label each cluster with the label whose representation in images space is nearest to cluster center.
\end{enumerate}
 Although the method above outperforms the state of the art on most zero-shot recognition benchmark,
its performance is bottlenecked with clustering accuracy. Because all members of each cluster is assumed to
share the same label. To overcome this bottleneck, in the next section we
present a joint frame work to learn cluster assignments jointly with the mapping.

We use a simple ridge regression to map class signatures to visual features. This method, though simple,
will serve well in our framework. The mapping should map a single class signature to various images and is penalized
for distance from each one, so it is learned in a way that does good \textit{on average}. This is specifically
favorable in our setting where we want to use this mapping to classify cluster centers that also
approximate \textit{average} of class instances.

The mapping is characterized by the following equation
\begin{equation}
  D = \argmin_D \norm{X_s - D Y_s} + \gamma \norm{D}
\end{equation}
where columns of $ Y_s \in \mathbb{R}^{r \times n_s} $  are class signatures of images in columns of $X_s$.
This equation is known to have the closed form solution:
\begin{equation} \label{eq:dic}
  D = X_s Y_s^T (Y_s Y_s^T + \gamma I)^{-1}
\end{equation}.
The parameter $\gamma$ is determined through cross validation which we will describe precisely in Sec.\ref{experiments}.

The clustering problem over unseen instances in zero-shot recognition is
 different from conventional semi-supervised clustering tasks \cite{}
in this way that unlabeled instances are completely disjoint from labeled ones.
Also converting labels to pairwise similarity and dissimilarity constraints among instances will impose
a huge number of constraints, this inhibits using pairwise constraint based clustering methods like \cite{}.
 So here we propose a semi-supervised clustering method which
can be seen as an extension to k-means and captures the special natures of our problem.
\begin{equation}
\minimize_{R, \mathbf{\mu_1, \ldots, \mu_k }}  \sum_{n,k} r_{nk} \lVert \mathbf{x_n - \mu_k} \rVert +
 \beta \sum_{n=1}^{N_s} \mathds{1}(\mathbf{r_n \neq z_n})
\end{equation}
where $\mathbf{\mu_i}$'s are cluster centers and $R = [\mathbf{r_1, \ldots, r_{N_s + N_u }} ]$ is cluster assignments in one-hot encoding format.
The objective function is similar to kmean's but for each labeled instance there is a penalty of $\beta$ if its assigned cluster number is different from its label thus encouraging
the first $n_s$ cluster be same as seen classes. This is intuitively plausible, on one hand all data is being used
in the clustering and on the other ????????

Parameters $\beta$ and $k$ can be determined through cross validation, however in our experiments we found out
the model not to be very sensitive to these parameters so we fix $\beta=1$ and $k =  (n_s + n_u)$ i.e. number of categories.

Finally to assign labels to test instances, we use the mapping $D$ from \eqref{eq:dic} to map class signatures to visual
features, creating a set of \textit{landmarks} in the images space. We then classify just the cluster centers using a
nearest neighbor classifier. All of instances in classifier follow up the label predicted for the cluster center.
We argue that the this method of assigning labels to clusters rather than individual instances can mitigate the
domain shift problem in zero-shot learning.
............

\section{Joint mapping and clustering}
\label{joint}
In the method we presented earlier, once the the points are assigned to a cluster,
they all will inevitably share an identical label. This can limit the performance of the method. One
example arises for cluster \textit{outliers} that are distant from center of their cluster but lie
nearby a landmark. This instances most probably belong to the class of nearby landmark but will be labeled same
as their cluster center. To overcome this issue we propose a framework to learn the mapping jointly with cluster assignments
 (which will be reported as label predictions at the end).
 While allowing more flexibility, this framework abates the domain shift problem in the same way as the previous method.
 The objective function is formulated as follows:
 \begin{align} \label{eq:main}
   \minimize_{R,D} \norm{X_s - D Y_s}  &+ \lambda \norm{X_t - D S_t R^T } + \gamma \norm{D} \\
   \text{s. t.} \quad & R \in \{0,1\}^{N_u \times n_u}
 \end{align}
 The second term in the objective penalizes tries to learn mapping $D$ and cluster assignments minimizing the
 distance between test instances and representation of their class signatures mapped by $D$. By enforcing
 the signatures to be mapped close to test instances this term mitigates the domain shift problem.
 The second term is essentially a clustering objective but with two advantages. First the number of clusters is no longer a
 parameter, it is determined by the number of unseen classes. Second, the cluster centers are bound to be
 mapping of test class signatures and the mapping also appears in the other terms.

\subsection{Optimization}
The Eq.\eqref{eq:main} is not convex and considering that $R$ is a partitioning the global optimization requires an
exhaustive search over all possible labeling of test data with $n_u$ labels. So we use a simple coordinate descent
method like in k-means. We alternate between optimization $R$ or $D$ while fixing the other.
Having fixed the labeling $R$, the problem becomes a simple multi-task ridge regression which has the closed form solution:
\begin{equation} \label{eq:d_update}
  D = (X_s Y_s^T + \beta X_u R S_u^T) (Y_s Y_s^T + \beta S_u R^T R S_u^T  + \gamma I)^{-1}
\end{equation}
Fixing $D$ the optimal $R$ can be achieved by assigning each instance to the closest landmark
\begin{equation} \label{eq:r_update}
  r_{ij} = \mathds{1}[j = \argmin_{k} \lVert X_{u(i)} - D S_{u(k)} \rVert_2 ]
\end{equation}
We continue alternating between updates of $D$ and $R$ till R remains constants i.e. no instance changes label. In our experiments this always happens
in less that 15 iterations.

We rely on good initialization to evade poor local minima.
We initialize  $R$ by final perditions from our first method.

\section{Experiments} \label{experiments}
\begin{table*}[ht]\footnotesize
\begin{minipage}{\textwidth}
\centering
\caption{\footnotesize{Zero-shot recognition accuracy comparison (\%) on aP\&Y, AwA, CUB-200-2011, and SUN Attribute, respectively, in the form of mean$\pm$standard deviation. Here except our results, the rest numbers are cited from their original papers. Note that some experimental settings may differ from ours.}}\label{tab:apy}\vspace{1mm}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Feature & Method & Animals with Attributes & CUB-2011 & aPascal \& aYahoo & SUN \\
\hline\hline
{Non-CNN}
& Li and Guo \cite{li15max}                 &  38.2$\pm$2.3   &                 &                         & 18.9$\pm$2.5 \\
& Li \etal~\cite{semi15}                    &  40.05$\pm$2.25 &                 &   24.71 $\pm$3.19       &     \\
\hline
\hline
{GoogleLeNet}
& Akata \etal~\cite{Akata2015}              & 66.7            & 50.1            &                         & \\
& Changpinyo \etal~\cite{Synthesized}       & 72.9            & 54.5            &                         & 62.7 \\
\hline
{vgg-19 \cite{vgg}}
& Khodirov \etal \cite{Kodirov2015}\footnote{Results using just attribute vectors as side information is listed here}
                                            & 73.2            &  39.5           & 26.5                    &  \\
& Akata \etal~\cite{Akata2015}              & 61.9            &  50.1           &                         & \\
& Zhang and Saligrama \cite{sse}            &  76.33$\pm$0.53 & 30.41 $\pm$0.20 &   46.23 $\pm$ 0.53      & 82.50 $\pm$ 1.32    \\
& Zhang and Saligrama \cite{agnostic}       &  80.46$\pm$0.53 & 42.11 $\pm$0.55 &   50.35 $\pm$ 2.97      & 83.83 $\pm$ 0.29    \\
& Ours (init D) & \textbf{\em 83.03$\pm$0.53} & \textbf{\em 59.39$\pm$0.83} & 49.41$\pm$0.20 & \textbf{\em 88.50$\pm$1.32} \\
& Ours (init R) & \textbf{\em 83.03$\pm$0.53} & \textbf{\em 59.39$\pm$0.83} & 49.41$\pm$0.20 & \textbf{\em 88.50$\pm$1.32} \\
%SSE-INT-INT & \textbf{\em 48.79} & 64.60 & 32.15 & \\
%SSE-INT-ReLU & 43.76 & 66.97 & \textbf{\em 32.70} & \\
\hline
\end{tabular}
\end{minipage}\vspace{-3mm}
\end{table*}

\textbf{Data Sets.}
We evaluate our proposed methods on four popular public benchmarks for zero-shot classification.
For all datasets we use $4096-$dimensional features from first fully connected layer of VGG19 network \cite{vgg}
provided publicly as supplementary material of \cite{sse}.
(1) Animal with Attributes (AwA) \cite{lampert09}. There are images of 50 mammal species in this data set
each class described by a single $85-$dimensional attribute vector. We use the continuous attributes rather than
the binary version as it yield better results. The train/test split provided by the dataset is used.
(2) aPascal/aYahoo \cite{farhadi09}. The 20 categories from Pascal VOC 2008 \cite{pascal} are considered seen classes and
categories from aYahoo are considered unseen. As this dataset provides instance level attribute vectors,
for class signatures we use the average of instance attributes.
(3) SUN Attribute \cite{sun}. The dataset consists of 717 categories and all images are annotated with 102 attributes, we just
use the average attributes among all instances of each categories for our experiments here. We use the same train/test spilit
as in \cite{jayaraman14} where 10 classes have been considered unseen.
(4) Caltech UCSD Birds-2011 (CUB) \cite{cub}. This a dataset for fine-grained classification task. There are 200 species of
birds each image annotated with 312 binary attributes, again we average over instances to get continuous class signatures.
We use the same train/test split as in \cite{akata13} (and many other following works) to make comparison possible.

To adjust parameters $\gamma$ and $\beta$ in Eq.~\ref{eq:d_update}, we split training data into train and validation sets.
We choose a number of categories randomly from training data as validation categories. For each data set, the size of
validation set has the same ratio to train set as the size of test categories to total of train and validation.
Once optimal $\gamma$ and $\beta$ are determined through grid search by testing on validation set, The model
is then trained on all seen categories.

We summarize our experimental results in \ref{tab:results}. For other methods we use the results reported in the original publication.
We did not re-implement any of the methods and if the original paper does not report on a data set we leave the corresponding cell blank.
Our method performs best on 3 out of 4 datasets except for aPascal/aYahoo. This can be explained by the nature of the dataset
in which class signatures obtained by averaging instance attributes are very similar. We suppose trying to learn
more discriminative signatures from data can potentially improve the result. We investigate this in our future work.

We implemented our method using scikit-learn library \cite{} in Python.
\footnote{Code is available at :-?}
 and used an Intel Core i5 CPU at 3.2 GHz to run the experiments.
On all datasets in takes less than 10 minutes from loading data to yielding predictions.

\section{Conclusion} \label{conclusion}
This is a conclusion and we conclude.
This is a conclusion and we conclude.
This is a conclusion and we conclude.
This is a conclusion and we conclude.
This is a conclusion and we conclude.
This is a conclusion and we conclude.
This is a conclusion and we conclude.
This is a conclusion and we conclude.
This is a conclusion and we conclude.
This is a conclusion and we conclude.

\section*{Acknowledgement}
{\small
\bibliographystyle{ieee}
\bibliography{../ref}
}
