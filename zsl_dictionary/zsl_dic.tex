%\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

%\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

% The booktabs package is used by this sample document
% (it provides \toprule, \midrule and \bottomrule).
% Remove the next line if you don't require it.
\usepackage{booktabs}
% The siunitx package is used by this sample document
% to align numbers in a column by their decimal point.
% Remove the next line if you don't require it.
%\usepackage[load-configurations=version-1]{siunitx} % newer version
%\usepackage{siunitx}
%\usepackage{natbib}

% The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

\jmlrvolume{60}
\jmlryear{2016}
\jmlrworkshop{ACML 2016}

\title[Short Title]{Full Title of Article}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \author{\Name{Author Name1} \Email{abc@sample.com}\and
 %  \Name{Author Name2} \Email{xyz@sample.com}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \author{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \Name{Author Name4} \Email{an4@sample.com}\\
 %  \Name{Author Name5} \Email{an5@sample.com}\\
 %  \Name{Author Name6} \Email{an6@sample.com}\\
 %  \Name{Author Name7} \Email{an7@sample.com}\\
 %  \Name{Author Name8} \Email{an8@sample.com}\\
 %  \Name{Author Name9} \Email{an9@sample.com}\\
 %  \Name{Author Name10} \Email{an10@sample.com}\\
 %  \Name{Author Name11} \Email{an11@sample.com}\\
 %  \Name{Author Name12} \Email{an12@sample.com}\\
 %  \Name{Author Name13} \Email{an13@sample.com}\\
 %  \Name{Author Name14} \Email{an14@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
  \author{\Name{Author Name1} \Email{abc@sample.com}\\
  \addr Address 1
  \AND
  \Name{Author Name2} \Email{xyz@sample.com}\\
  \addr Address 2
 }

%\editors{List of editors' names}

\begin{document}

\maketitle

\begin{abstract}
This is the abstract for this article.
\end{abstract}
\begin{keywords}
List of keywords
\end{keywords}

\section{Introduction}
Zero-shot learning is an extension to the conventional supervised learning scenario
that introduces task of recognizing instances from categories with no training sample.
 The problem addressed by zero-shot learning, rises naturally in practice wherever acquiring ample labeled instances
 is not feasible for all categories. Examples include large-scale and fine-grained classification.
Describing the task more precisely, in training phase labeled samples from some categories are provided,
these categories are called seen categories. There are also some other categories without labeled instances, these are called unseen categories.
There also exists some sort of description for every category that are sometimes called \textit{class signatures}.
Signatures may be a set of human-annotated discriminative attributes or derived from textual description available online.
In the test phase unlabeled instances should be classified into seen or unseen categories. In this work, however we focus on the more popular version
in which test instances belong only to unseen categories.
 %In this setting, zero-shot learning mitigates the problem of acquiring labeled instances, which can be hard for novel, rare and fine grained categories.

Most existing methods for zero-shot learning focus on using labeled images to learn a compatibility function indicating how similar an image is
to each label embedding. Each instance will then be labeled with the category having the most compatible signature. Little attention
has been paid to exploiting the information available in the distribution of images themselves.

 In recent years, advances in deep convoloutional neural networks provides rich visual features with high discrimination capability.
 Thus there is usually a meaningful structure in visual features domain
that can be used to help classification be done more precisely, like in semi-supervised settings. We will show through experiment that
the space of images is indeed a rich space in which instances categories form natural clusters.
 %In the previous works, most concentration has been on finding embeddings for visual features and little effort has been made to exploit unsupervised information in images domain.

 In this work
 %we first justify the claim made above by applying a simple clustering algorithm and reporting
%Rand Index measure between cluster assignments and ground truth labels. Motivated by the result,
 we first propose a
  semi-supervised clustering algorithm to leverage labels available for seen classes to obtain more accurate cluster assignments.
 Then we use a simple dictionary learning method to map class descriptions to visual features, creating a set of landmarks in
 visual domain. These landmarks are used to label cluster centers driven from clustering algorithm. We argue that this learning
 scheme can mitigate domain shift problem \cite{eccv14} between seen and unseen categories and thus achieving higher classification accuracy.
In Sec. \ref{experiments} we present experimental results on four popular zero-shot classification benchmarks to see
the method above outperforms the state of the art on three of them.
 We extend our work further and propose a framework to learn the dictionary and cluster (class label) assignments jointly
 improving results obtained from our first method.

The rest of paper is organized as follow...
\section{Related Work}
Most existing methods for zero-shot object recognition can be described as finding a compatibility function or
or with more fine-granularity as follows:
\begin{itemize}
  \item find (or use existing) embeddings for class labels in a semantic space
  \item map images into the semantic space
  \item classify images in the semantic space using nearest neighbors or label propagation
\end{itemize}
these steps may be done independently or jointly.

In methods based on attribute prediction \cite{ziad} the semantic space is the space of externally provided attributes so the label embeddings are
already available and images are embedded in that space using attribute predictors. A relatively large body of work in zero-shot recognition is
dedicated to attribute prediction, early methods like \cite{lamplampert09} assume independence between attributes and train binary attribute classifiers.
Probabilistic graphical models have been used to model and/or learn correlations among different attributes (\cite{}), to improve the prediction.
In \cite{jayaraman14}  a random forest approach has been employed that accounts for unreliability in attribute predictions
 for final class assignment.

%transfer learning in transductive settings

More recent works, exploit bilinear mapping to embed images into semantic space and assign a compatibility score to image and label embedding
 pairs based on their inner product. Different objective functions have been proposed for learning such bilinear mapping. \cite{emb15}
 proposed a simple sum of squared error but with better regularization.
\cite{li15max} define a max-margin objective function similar to structured SVM objective.
 to find the mapping. Authors of \cite{li15max, semi15} also use a max margin objective function
while the latter does not fix the label embeddings to externally provided ones in order to obtain more discriminative label embeddings. These two methods
differ from other existing methods in this way that embeddings and labels for test instances are learned simultaneously. This provides the possibility of
leveraging unsupervised information in test images for example as done in \cite{semi15} by using a Laplacian regularization term that penalizes
 similar objects assigned to different classes.
 An important distinction between these two methods and our works is that we use an explicit clustering on unseen instances
 and label prediction is done for the whole cluster. This will significantly reduce the number of parameters
 and simplify the learning algorithm resulting in faster optimization.
 % and ability to use more discriminative high-dimensional deep features.

 In absence of human annotated attributes, text from online encyclopedias or just the name of the class is used. Some existing method
 first do a feature extraction on these auxiliary information turning them to vectors that can be treated analogous to attribute vectors.
 \cite{devise} use a bilinear mapping as compatibility function among deep visual features and Word2vec \cite{word2vec} representation of
 class names. \cite{ba2015} uses a nonlinear mapping defined by a neural network to model compatibility. \cite{mohamed13} combines regression with knowledge transfer and proposes an objective to predict classifier parameters
 for unseen categories .

 Designing label embedding is another line of research that can also be used for zero-shot recognition. For a label embedding to be helpful in classification task,
 it should exhibit two properties \cite{Yu2013}: 1) category-separability 2) Learnability. In \cite{Yu2013} an objective function is defined to derive
 such label embeddings based on information about similarities among categories.

A relatively popular embedding for labels is the to describe unseen categories as how similar they are to seen ones.
\cite{costa} construct  classifier for an unseen category by linear combination of seen class classifiers
using this kind of representation as mixing weights. \cite{convex} use output from softmax layer of a CNN to get the similarity scores.
Using these as weights they represent images in the semantic space as convex combination of seen label embeddings.
\cite{sse} also uses histogram of seen class proportions as label embedding and then define a max margin framework to embed images
 in this space. The authors extend their work further \cite{agnostic} and formulate a supervised dictionary learning
 for learning embeddings jointly.


There are major differences between our work and  ~\cite{Kodirov2015} which also uses a dictionary learning scheme in which coding coefficients is considered to be
 label embeddings in semantic space. In that work a sparse coding objective with two regularization terms designed specifically for
 zero-shot learning are used to map images into semantic space and class labels are assigned to them using label propagation. We use
 the standard sparse coding objective and use the dictionary learned to map unseen class signatures to visual feature space and then
 use them to assign labels to pre-computed cluster centers.

\section{Proposed Approach [Or a name maybe :-?]}
In this section we first ...

\subsection{An Observation about Data Distribution}
In this section we run a simple experiment on public zero-shot classification benchmarks to justify our
postulate about distribution of data exhibiting rich clustering property. To this end we run k-means clustering
algorithm on test instances with number of clusters varying between number of unseen instances and double of that.
We report rand index score of cluster assignments attained comparing to ground truth labels.
To make the results more telling we conduct another experiment. We split instances form unseen categories
to a 90\%-10\% split train SVMs with RBF kernel on the first split. Classification accuracy on the second split is reported in table \ref{tab}.
It can be seen that blah blah.%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\subsection{Notation}

An figure in Fig.~\ref{fig:spiral}
\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.5\textwidth]{../images/logo.pdf}
\caption{A spiral.}\label{fig:spiral}
\end{center}
\end{figure}

An example of citation~\cite{DBLP:conf/acml/2009}.

\acks{Acknowledgements should go at the end, before appendices and references.}

%\bibliographystyle{plain}
\bibliography{ref}

\appendix

\section{First Appendix}\label{apd:first}

This is the first appendix.

\section{Second Appendix}\label{apd:second}

This is the second appendix.


\end{document}
