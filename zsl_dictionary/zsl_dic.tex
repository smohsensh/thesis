%\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

%\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

% The booktabs package is used by this sample document
% (it provides \toprule, \midrule and \bottomrule).
% Remove the next line if you don't require it.
\usepackage{booktabs}
% The siunitx package is used by this sample document
% to align numbers in a column by their decimal point.
% Remove the next line if you don't require it.
%\usepackage[load-configurations=version-1]{siunitx} % newer version
%\usepackage{siunitx}
%\usepackage{natbib}

% The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

\jmlrvolume{60}
\jmlryear{2016}
\jmlrworkshop{ACML 2016}

\title[Short Title]{Full Title of Article}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \author{\Name{Author Name1} \Email{abc@sample.com}\and
 %  \Name{Author Name2} \Email{xyz@sample.com}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \author{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \Name{Author Name4} \Email{an4@sample.com}\\
 %  \Name{Author Name5} \Email{an5@sample.com}\\
 %  \Name{Author Name6} \Email{an6@sample.com}\\
 %  \Name{Author Name7} \Email{an7@sample.com}\\
 %  \Name{Author Name8} \Email{an8@sample.com}\\
 %  \Name{Author Name9} \Email{an9@sample.com}\\
 %  \Name{Author Name10} \Email{an10@sample.com}\\
 %  \Name{Author Name11} \Email{an11@sample.com}\\
 %  \Name{Author Name12} \Email{an12@sample.com}\\
 %  \Name{Author Name13} \Email{an13@sample.com}\\
 %  \Name{Author Name14} \Email{an14@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
  \author{\Name{Author Name1} \Email{abc@sample.com}\\
  \addr Address 1
  \AND
  \Name{Author Name2} \Email{xyz@sample.com}\\
  \addr Address 2
 }

%\editors{List of editors' names}

\begin{document}

\maketitle

\begin{abstract}
This is the abstract for this article.
\end{abstract}
\begin{keywords}
List of keywords
\end{keywords}

\section{Introduction}
Zero-shot learning is an extension to the conventional supervised learning scenario
that introduces the task of recognizing instances from categories with no training sample.
To describe the task more precisely, in training phase labeled samples from some categories are provided,
these categories are called seen categories. Also for each category some sort of description is available.
Description may be human annotated attributes or computed from textual description available online.
There are also some other categories without labeled instances, these are called unseen categories.
 In test time, unlabeled instances should be classified into unseen classes.
 In this setting, zero-shot learning mitigates the problem of acquiring labeled instances, which can be hard for novel, rare and fine grained categories.

 In recent years, advances in deep convoloutional neural networks provides rich visual features with high discrimination capability.
 Thus there is usually a meaningful structure in visual features domain
that can be used to help classification be done more precisely like in semi-supervised settings.
 In the previous works, most concentration has been on finding embeddings for visual features
 so that they're mapped near their true description and far from descriptions of other classes.
 little attention has been paid to using embeddings for --------------------------------------------------------------


\section{Related Work}
Most existing methods for zero-shot object recognition can be described as finding a compatibility function or
or with more fine-granularity as follows:
\begin{itemize}
  \item find (or use existing) embeddings for class labels in a semantic space
  \item map images into the semantic space
  \item classify images in the semantic space using nearest neighbors or label propagation
\end{itemize}
these steps may be done independently or jointly.

In methods based on attribute prediction \cite{ziad} the semantic space is the space of externally provided attributes so the label embeddings are
already available and images are embedded in that space using attribute predictors. A relatively large body of work in zero-shot recognition is
dedicated to attribute prediction, early methods like \cite{lamp} assume independence between attributes and train binary attribute classifiers.
Probabilistic graphical models have been used to model and/or learn correlations among different attributes \cite{}, to improve the prediction.
In \cite{jayaraman14}  a random forest approach has been employed that accounts for unreliability in attribute predictions for final class assignment.

Designing label embedding is another line of research that can also be used for zero-shot recognition. For a label embedding to be helpful in classification task,
it should exhibit two properties \cite{Yu2013}: 1) category-separability 2) Learnability. In \cite{Yu2013} an objective function is defined to derive
such label embeddings based on information about similarities among categories.

sse , agnostic

More recent works, exploit bilinear mapping to embed images into semantic space and assign a compatibility score to image and label embedding pairs.
\cite{li15max} defines a max-margin objective function to find the mapping. Authors of \cite{max, semi15} also use a max margin objective function
while the latter does not fix the label embeddings to externally provided ones in order to obtain more discriminative label embeddings. These two methods
differ from other existing methods in this way that embeddings and labels for test instances are learned simultaneously. This provides the possibility of
leveraging unsupervised information in test images for example as done in \cite{semi15} by using a Laplacian regularization term that penalizes
 similar objects assigned to different classes.
 An important distinction between these two methods and our works is that we use an explicit clustering on unseen instances and label prediction is done
 for the whole cluster. This will significantly reduce the number of parameters and simplify the learning algorithm resulting in faster optimization.
 % and ability to use more discriminative high-dimensional deep features.

 Most similar to out method is \cite{Kodirov2015} which also uses a dictionary learning scheme. Semantic representation of each class is considered as its
 coding coefficients and 
  with two new regularization terms, one acting as
 a domain adaptation

An figure in Fig.~\ref{fig:spiral}
\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.5\textwidth]{../logo.pdf}
\caption{A spiral.}\label{fig:spiral}
\end{center}
\end{figure}

An example of citation~\cite{DBLP:conf/acml/2009}.

\acks{Acknowledgements should go at the end, before appendices and references.}

%\bibliographystyle{plain}
\bibliography{ref}

\appendix

\section{First Appendix}\label{apd:first}

This is the first appendix.

\section{Second Appendix}\label{apd:second}

This is the second appendix.


\end{document}
