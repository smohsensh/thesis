%\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}

\usepackage{dsfont}
% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

%\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

% The booktabs package is used by this sample document
% (it provides \toprule, \midrule and \bottomrule).
% Remove the next line if you don't require it.
\usepackage{booktabs}
% The siunitx package is used by this sample document
% to align numbers in a column by their decimal point.
% Remove the next line if you don't require it.
%\usepackage[load-configurations=version-1]{siunitx} % newer version
%\usepackage{siunitx}
%\usepackage{natbib}

% The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

\jmlrvolume{60}
\jmlryear{2016}
\jmlrworkshop{ACML 2016}

\title[Short Title]{Full Title of Article}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \author{\Name{Author Name1} \Email{abc@sample.com}\and
 %  \Name{Author Name2} \Email{xyz@sample.com}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \author{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \Name{Author Name4} \Email{an4@sample.com}\\
 %  \Name{Author Name5} \Email{an5@sample.com}\\
 %  \Name{Author Name6} \Email{an6@sample.com}\\
 %  \Name{Author Name7} \Email{an7@sample.com}\\
 %  \Name{Author Name8} \Email{an8@sample.com}\\
 %  \Name{Author Name9} \Email{an9@sample.com}\\
 %  \Name{Author Name10} \Email{an10@sample.com}\\
 %  \Name{Author Name11} \Email{an11@sample.com}\\
 %  \Name{Author Name12} \Email{an12@sample.com}\\
 %  \Name{Author Name13} \Email{an13@sample.com}\\
 %  \Name{Author Name14} \Email{an14@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
  \author{\Name{Author Name1} \Email{abc@sample.com}\\
  \addr Address 1
  \AND
  \Name{Author Name2} \Email{xyz@sample.com}\\
  \addr Address 2
 }

%\editors{List of editors' names}
\newcommand{\norm}[1]{\left \lVert #1 \right \rVert_{F}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\minimize}{minimize}
\begin{document}

\maketitle

\begin{abstract}
This is the abstract for this article.
\end{abstract}
\begin{keywords}
List of keywords
\end{keywords}

\section{Introduction}
Zero-shot learning is an extension to the conventional supervised learning scenario
that introduces task of recognizing instances from categories with no training sample.
 The problem addressed by zero-shot learning, rises naturally in practice wherever acquiring ample labeled instances
 is not feasible for all categories. Examples include large-scale and fine-grained classification.
Describing the task more precisely, in training phase labeled samples from some categories are provided,
these categories are called seen categories. There are also some other categories without labeled instances, these are called unseen categories.
There also exists some sort of description for every category that are sometimes called \textit{class signatures}.
Signatures may be a set of human-annotated discriminative attributes or derived from textual description available online.
In the test phase unlabeled instances should be classified into seen or unseen categories. In this work, however we focus on the more popular version
in which test instances belong only to unseen categories.
 %In this setting, zero-shot learning mitigates the problem of acquiring labeled instances, which can be hard for novel, rare and fine grained categories.

Most existing methods for zero-shot learning focus on using labeled images to learn a compatibility function indicating how similar an image is
to each label embedding. Each instance will then be labeled with the category having the most compatible signature. Little attention
has been paid to exploiting the information available in the distribution of images themselves.

 In recent years, advances in deep convoloutional neural networks provides rich visual features with high discrimination capability.
 Thus there is usually a meaningful structure in visual features domain
that can be used to help classification be done more precisely, like in semi-supervised settings. We will show through experiment that
the space of images is indeed a rich space in which instances categories form natural clusters.
 %In the previous works, most concentration has been on finding embeddings for visual features and little effort has been made to exploit unsupervised information in images domain.

 In this work
 %we first justify the claim made above by applying a simple clustering algorithm and reporting
%Rand Index measure between cluster assignments and ground truth labels. Motivated by the result,
 we first propose a
  semi-supervised clustering algorithm to leverage labels available for seen classes to obtain more accurate cluster assignments.
 Then we use a simple dictionary learning method to map class descriptions to visual features, creating a set of landmarks in
 visual domain. These landmarks are used to label cluster centers driven from clustering algorithm. We argue that this learning
 scheme can mitigate domain shift problem \cite{eccv14} between seen and unseen categories and thus achieving higher classification accuracy.
In Sec. \ref{experiments} we present experimental results on four popular zero-shot classification benchmarks to see
the method above outperforms the state of the art on three of them.
 We extend our work further and propose a framework to learn the dictionary and cluster (class label) assignments jointly
 improving results obtained from our first method.

The rest of paper is organized as follow...
\section{Related Work}
Most existing methods for zero-shot object recognition can be described as finding a compatibility function or
or with more fine-granularity as follows:
\begin{itemize}
  \item find (or use existing) embeddings for class labels in a semantic space
  \item map images into the semantic space
  \item classify images in the semantic space using nearest neighbors or label propagation
\end{itemize}
these steps may be done independently or jointly.

In methods based on attribute prediction \cite{ziad} the semantic space is the space of externally provided attributes so the label embeddings are
already available and images are embedded in that space using attribute predictors. A relatively large body of work in zero-shot recognition is
dedicated to attribute prediction, early methods like \cite{lamplampert09} assume independence between attributes and train binary attribute classifiers.
Probabilistic graphical models have been used to model and/or learn correlations among different attributes (\cite{}), to improve the prediction.
In \cite{jayaraman14}  a random forest approach has been employed that accounts for unreliability in attribute predictions
 for final class assignment.

%transfer learning in transductive settings

More recent works, exploit bilinear mapping to embed images into semantic space and assign a compatibility score to image and label embedding
 pairs based on their inner product. Different objective functions have been proposed for learning such bilinear mapping. \cite{emb15}
 proposed a simple sum of squared error but with better regularization.
\cite{li15max} define a max-margin objective function similar to structured SVM objective.
 to find the mapping. Authors of \cite{li15max, semi15} also use a max margin objective function
while the latter does not fix the label embeddings to externally provided ones in order to obtain more discriminative label embeddings. These two methods
differ from other existing methods in this way that embeddings and labels for test instances are learned simultaneously. This provides the possibility of
leveraging unsupervised information in test images for example as done in \cite{semi15} by using a Laplacian regularization term that penalizes
 similar objects assigned to different classes.
 An important distinction between these two methods and our works is that we use an explicit clustering on unseen instances
 and label prediction is done for the whole cluster. This will significantly reduce the number of parameters
 and simplify the learning algorithm resulting in faster optimization.
 % and ability to use more discriminative high-dimensional deep features.

 In absence of human annotated attributes, text from online encyclopedias or just the name of the class is used. Some existing method
 first do a feature extraction on these auxiliary information turning them to vectors that can be treated analogous to attribute vectors.
 \cite{devise} use a bilinear mapping as compatibility function among deep visual features and Word2vec \cite{word2vec} representation of
 class names. \cite{ba2015} uses a nonlinear mapping defined by a neural network to model compatibility. \cite{mohamed13} combines regression with knowledge transfer and proposes an objective to predict classifier parameters
 for unseen categories .

 Designing label embedding is another line of research that can also be used for zero-shot recognition. For a label embedding to be helpful in classification task,
 it should exhibit two properties \cite{Yu2013}: 1) category-separability 2) Learnability. In \cite{Yu2013} an objective function is defined to derive
 such label embeddings based on information about similarities among categories.

A relatively popular embedding for labels is the to describe unseen categories as how similar they are to seen ones.
\cite{costa} construct  classifier for an unseen category by linear combination of seen class classifiers
using this kind of representation as mixing weights. \cite{convex} use output from softmax layer of a CNN to get the similarity scores.
Using these as weights they represent images in the semantic space as convex combination of seen label embeddings.
\cite{sse} also uses histogram of seen class proportions as label embedding and then define a max margin framework to embed images
 in this space. The authors extend their work further \cite{agnostic} and formulate a supervised dictionary learning
 for learning embeddings jointly.


There are major differences between our work and  ~\cite{Kodirov2015} which also uses a dictionary learning scheme in which coding coefficients is considered to be
 label embeddings in semantic space. In that work a sparse coding objective with two regularization terms designed specifically for
 zero-shot learning are used to map images into semantic space and class labels are assigned to them using label propagation. We use
 the standard sparse coding objective and use the dictionary learned to map unseen class signatures to visual feature space and then
 use them to assign labels to pre-computed cluster centers.

\section{Proposed Approach [Or a name maybe :-?]}
In this section we first ...


\subsection{Notation}
$X, \mathbf{x}, x$ denote matrices, column vectors and scalars respectively. $\norm{X}$ denotes the Frobnious norm of a matrix.
Suppose there are $n_s$ seen categories and $n_u$ unseen categories. For each category y,
auxiliary information $a_y \in \mathcal{R}^r$ is available. We assume that labels $\{1, \ldots, n_s \}$
correspond to seen categories.

We use $X_s \in \mathbb{R}^{d \times N_s}$ and $X_u \in \mathbb{R}^{d \times N_u}$
to denote matrices whose columns are seen and unseen images respectively where $d$ is the dimension of image features.
$S_s = [a_1, \ldots, a_{n_s}]$ denotes the matrix of seen class signatures. $S_u$ is defined similarly. $Y_s$
denotes train data labels in one hot encoding format.

\subsection{Clustering Method}
Our first method can be roughly summarized in three steps:
\begin{enumerate}
  \item Using data from seen classes we learn a linear mapping from  attribute vectors to images like in \cite{Kodirov2015}
  \item We cluster data using our proposed clustering algorithm
  \item We then label each cluster with the label whose representation in images space is nearest to cluster center.
\end{enumerate}
 Although the method above outperforms the state of the art on most zero-shot recognition benchmark,
its performance is bottlenecked with clustering accuracy, Because all members of each cluster is assumed to
share the same label. To overcome this bottleneck, in the next section we
present a joint frame work to learn cluster assignments jointly with the mapping.

We use a simple ridge regression to map class signatures to visual features. This method, though simple,
will serve well in our method. The mapping should map a single class signature to various images and is penalized
for distance from each one, so it is learned in a way that does good \textit{on average}. This is ***ly
favorable in our setting where we want to use this mapping to classify cluster centers that also
approximate \textit{average} of class instances.

The mapping is characterized by the following equation
\begin{equation}
  D = \argmin_D \norm{X_s - D Y_s} + \gamma \norm{D}
\end{equation}
where columns of $ Y_s \in \mathcal{R}^{r \times n_s} $  are class signatures of images in columns of $X_s$.
This equation is known to have closed form solution:
\begin{equation} \label{eq:dic}
  D = X_s Y_s^T (Y_s Y_s^T + \gamma I)^{-1}
\end{equation}.
The parameter $\gamma$ is determined through cross validation of reconstruction loss. We found out through experiment that
this will do almost as good as the more sophisticated
 cross validation on final classification accuracy in which some seen categories are considered are omitted
 from train data to act as validation classes.

The clustering problem here is different from conventional semi-supervised clustering tasks \cite{}
in this way that unlabeled instances are completely disjoint from labeled ones. Here we propose an objective function which
can be seen as an extension to k-means and captures the special nature of clustering we need.
\begin{equation}
\minimize_{R, \mathbf{\mu_1, \ldots \mu_k }}  \sum_{n,k} r_{nk} \lVert \mathbf{x_n - \mu_k} \rVert +
 \beta \sum_{n=1}^{N_s} \mathds{1}(\mathbf{r_n \neq y_n})
\end{equation}

For each labeled instance there is a penalty of $\beta$ if its assigned cluster number is different from its label thus encouraging
the first $n_s$ cluster be same as seen classes. This is intuitively plausible, on one hand all data is being used
in the clustering and on the other ????????

Parameters $\beta$ and $k$ can be determined through cross validation, however in our experiments we found out
the model not to be very sensitive to these parameters so we fix $\beta=1$ and $k =  (n_s + n_u)$ i.e. number of categories.

Finally to assign labels to test instances, we use the mapping $D$ from \eqref{eq:dic} to map class signatures to visual
features, creating a set of \textit{landmarks} in the images space. We then classify just the cluster centers using a
nearest neighbor classifier. All of instances in classifier follow up the label predicted for the cluster center.

We argue that the this method of assigning labels to clusters rather than individual instances can mitigate the
domain shift problem in zero-shot learning.
............

\section{Joint mapping and clustering}
In the method we presented earlier, once the the points are assigned to a cluster,
they all will inevitably share an identical label. This can limit the performance of the method. One
example happen arises for cluster \textit{outliers} that are distant from center of their cluster but lie
nearby a landmark. This instances most probably belong to the class of nearby landmark but will be labeled same
as their cluster center. To overcome this issue we propose a framework to learn the mapping jointly with cluster assignments
 (which will be reported as label predictions at the end).
 While allowing more flexibility, this framework abates the domain shift problem in the same way as the previous method.
 The objective function is formulated as follows:
 \begin{align} \label{eq:main}
   \minimize_{R,D} \norm{X_s - D Y_s}  + \lambda \norm{X_t - D S_t R^T } + \gamma \norm{D} //
   s.t. \in \{0,1\}^{N_u \times n_u}
 \end{align}
 The second term in the objective penalizes tries to learn mapping $D$ and cluster assignments minimizing the
 distance between test instances and representation of their class signatures mapped by $D$. By enforcing
 the signatures to be mapped close to test instances this term mitigates the domain shift problem.
 The second term is essentially a clustering objective but with two advantages. First the number of clusters is no longer a
 parameter, it is determined by the number of unseen classes. Second, the cluster centers are bound to be
 mapping of test class signatures and the mapping also appears in the other terms.

\subsection{Optimization}
The \eqref{eq:main} is not convex and considering that $R$ is a partitioning the global optimization requires an
exhaustive search over all possible labeling of test data with $n_u$ labels. So we use a simple coordinate descent
method like in k-means. We alternate between optimization $R$ or $D$ while fixing the other.
Having fixed the labeling $R$, the problem becomes a simple multi-task ridge regression which has the closed form solution:
\begin{equation} \label{eq:d_update}
  D = (X_s Y_s^T + \beta X_u R S_u^T) (Y_s Y_s^T + \beta S_u R^T R S_u^T  + \gamma I)^{-1}
\end{equation}
Fixing $D$ the optimal $R$ can be achieved by assigning each instance to the closest landmark
\begin{equation} \label{eq:r_update}
  r_{ij} = \mathds{1}[j = \argmin_{k} \lVert X_{u(i)} - D S_{u(k)} \rVert_2 ]
\end{equation}

We rely on good initialization to evade poor local minima.
To initialize $D$ we use
\eqref{eq:dic} and $R$ is initialized with final perditions from our first method.


\section{Experiments}
\textbf{Data Sets.}
We evaluate our proposed methods on four popular public benchmarks for zero-shot classification.
For all datasets we use $4096-$dimensional features from first fully connected layer of VGG19 network \cite{vgg}
provided publicly as supplementary material of \cite{sse}.
(1) Animal with Attributes (AwA) \cite{lampert09}. There are images of 50 mammal species in this data set
each class described by a single $85-$dimensional attribute vector. We use the continuous attributes rather than
the binary version as it yield better results. The train/test split provided by the dataset is used.
(2) aPascal/aYahoo \cite{farhadi09}. The 20 categories from Pascal VOC 2008 \cite{pascal} are considered seen classes and
categories from aYhoo are considered unseen. As this dataset provides instance level attribute vectors,
for class signatures we use the average of instance attributes.
(3) SUN Attribute \cite{sun}. The dataset consists of 717 categories and all images are annotated with 102 attributes, we just
use the average attributes among all instances of each categories for our experiments here. We use the same train/test spilit
as in \cite{jayaraman14} where 10 classes have been considered unseen.
(4) Caltech UCSD Birds-2011 (CUB) \cite{cub}. This a dataset for fine-grained classification task. There are 200 species of
birds each image annotated with 312 binary attributes, again we average over instances to get continuous class signatures.
We use the same train/test split as in \cite{akata13} to make comparison.

To adjust parameters $\gamma$ and $\beta$ in Eq.~\ref{eq:d_update}, we split training data into two train and validation sets.
We choose a number of categories randomly from training data as validation categories. For each data set size of
validation set has the same ratio to train set as size of test categories to all of train and validation size.
Once optimal $\gamma$ and $\beta$ are determined through grid search by testing on validation set, The model
is then trained on all seen categories.

%
% An figure in Fig.~\ref{fig:spiral}
% \begin{figure}[htp]
% \begin{center}
% \includegraphics[width=0.5\textwidth]{../images/logo.pdf}
% \caption{A spiral.}\label{fig:spiral}
% \end{center}
% \end{figure}
%
% An example of citation~\cite{DBLP:conf/acml/2009}.

\acks{Acknowledgements should go at the end, before appendices and references.}

%\bibliographystyle{plain}
\bibliography{ref}

\appendix

\section{An Observation about Data Distribution}\label{observation}
In this section we run a simple experiment on public zero-shot classification benchmarks to justify our
postulate about distribution of data exhibiting rich clustering property. To this end we run k-means clustering
algorithm on test instances with number of clusters varying between number of unseen instances and double of that.
We report rand index score of cluster assignments attained comparing to ground truth labels.
To make the results more telling we conduct another experiment. We split instances form unseen categories
to a random 90\%-10\% split train SVMs with RBF kernel on the first split. Classification accuracy on the second split is reported in table \ref{tab}.
It can be seen that blah blah.%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\section{Second Appendix}\label{apd:second}

This is the second appendix.


\end{document}
